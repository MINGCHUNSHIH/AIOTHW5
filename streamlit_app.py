import streamlit as st
import joblib
import numpy as np
import os
import requests


def download_file(url: str, dest_path: str):
    """Download a file from `url` to `dest_path` with a simple progress indicator."""
    os.makedirs(os.path.dirname(dest_path), exist_ok=True)
    resp = requests.get(url, stream=True, timeout=30)
    resp.raise_for_status()
    total = int(resp.headers.get("content-length", 0))
    chunk_size = 8192
    downloaded = 0
    with open(dest_path + ".part", "wb") as f:
        for chunk in resp.iter_content(chunk_size=chunk_size):
            if not chunk:
                continue
            f.write(chunk)
            downloaded += len(chunk)
    os.replace(dest_path + ".part", dest_path)


@st.cache_resource
def load_model():
    model_path = os.path.join("models", "ai_detector.joblib")
    return joblib.load(model_path)


st.set_page_config(
    page_title="AI / Human æ–‡ç« åµæ¸¬å™¨",
    page_icon="ğŸ¤–",
)

st.title("ğŸ¤– AI vs Human æ–‡ç« åµæ¸¬å™¨")
st.write("è¼¸å…¥ä¸€æ®µæ–‡å­—ï¼Œæ¨¡å‹æœƒä¼°è¨ˆå®ƒæ˜¯ **AI ç”Ÿæˆ** é‚„æ˜¯ **äººé¡æ’°å¯«**ã€‚")

# Load model if available
model = None
model_path = os.path.join("models", "ai_detector.joblib")
try:
    model = load_model()
except Exception as e:
    st.warning(f"æ¨¡å‹è¼‰å…¥å¤±æ•—æˆ–ä¸å­˜åœ¨ï¼š{e}")

    # Provide option to download a pre-trained model from a URL (useful for Streamlit Cloud)
    st.info("è‹¥ä½ æ²’æœ‰æœ¬åœ°æ¨¡å‹ï¼Œå¯å¾é ç«¯ä¸‹è¼‰æ¨¡å‹æª”ã€‚")

    # Determine model URL: prefer Streamlit secrets, then environment variable, then placeholder
    MODEL_URL = None
    try:
        MODEL_URL = st.secrets.get("MODEL_URL")
    except Exception:
        MODEL_URL = None
    if not MODEL_URL:
        MODEL_URL = os.environ.get("MODEL_URL")

    if MODEL_URL:
        st.write("æ¨¡å‹ä¸‹è¼‰åœ°å€å·²è¨­å®šã€‚ä½ å¯ä»¥æŒ‰ä¸‹æŒ‰éˆ•ä¸‹è¼‰æ¨¡å‹ä¸¦è¼‰å…¥ã€‚")
        if st.button("ä¸‹è¼‰ä¸¦è¼‰å…¥æ¨¡å‹"):
            try:
                with st.spinner("æ­£åœ¨ä¸‹è¼‰æ¨¡å‹..."):
                    download_file(MODEL_URL, model_path)
                st.success("æ¨¡å‹ä¸‹è¼‰å®Œæˆï¼Œå·²å„²å­˜è‡³ models/ai_detector.joblibã€‚è«‹é‡æ–°æ•´ç†é é¢ä»¥è¼‰å…¥æ¨¡å‹ã€‚")
            except Exception as e2:
                st.error(f"ä¸‹è¼‰æ¨¡å‹å¤±æ•—ï¼š{e2}")
    else:
        st.write("æœªè¨­å®šæ¨¡å‹ä¸‹è¼‰åœ°å€ã€‚è«‹åœ¨ Streamlit secrets æˆ–ç’°å¢ƒè®Šæ•¸ `MODEL_URL` ä¸­æ”¾å…¥æ¨¡å‹æª”æ¡ˆçš„å¯ä¸‹è¼‰ URLï¼ˆä¾‹å¦‚ GitHub Release é€£çµï¼‰ã€‚")
        st.write("æˆ–æ˜¯åœ¨æœ¬åœ°å…ˆåŸ·è¡Œ `python src/train_model.py` ç”¢ç”Ÿ `models/ai_detector.joblib`ã€‚")


# User input
default_text = "è«‹åœ¨é€™è£¡è²¼ä¸Šä½ æƒ³æª¢æ¸¬çš„æ–‡ç« å…§å®¹..."

user_text = st.text_area(
    "è¼¸å…¥æˆ–è²¼ä¸Šæ–‡å­—ï¼š",
    height=220,
    placeholder=default_text
)

if st.button("é–‹å§‹åˆ†æ"):
    if not user_text.strip():
        st.warning("è«‹å…ˆè¼¸å…¥ä¸€äº›æ–‡å­—å†æŒ‰ä¸‹æŒ‰éˆ•ã€‚")
    else:
        if model is None:
            st.error("æ¨¡å‹å°šæœªè¼‰å…¥ã€‚è«‹å…ˆåŸ·è¡Œ `python src/train_model.py` ä¸¦ç¢ºèª `models/ai_detector.joblib` å­˜åœ¨ã€‚")
        else:
            proba = model.predict_proba([user_text])[0]
            classes = model.classes_

            # æ‰¾å‡º "AI" é€™ä¸€é¡çš„ä½ç½®ï¼Œå¦‚æœæ‰¾ä¸åˆ°å°±ç”¨ index 0 ä½œç‚º fallback
            try:
                ai_idx = int(np.where(classes == "AI")[0][0])
                ai_prob = float(proba[ai_idx])
            except Exception:
                ai_prob = float(proba[0])

            human_prob = 1.0 - ai_prob

            col1, col2 = st.columns(2)
            with col1:
                st.metric("AI ç”Ÿæˆçš„æ©Ÿç‡", f"{ai_prob * 100:.1f} %")
            with col2:
                st.metric("Human æ’°å¯«çš„æ©Ÿç‡", f"{human_prob * 100:.1f} %")

            st.write("---")
            st.write("ğŸ” **è¦–è¦ºåŒ–ï¼šAI æ©Ÿç‡æ¢**")
            st.progress(ai_prob)

            with st.expander("é¡¯ç¤ºæ¨¡å‹å…§éƒ¨è³‡è¨Šï¼ˆé¸ç”¨ï¼‰"):
                st.write("æ¨¡å‹ä½¿ç”¨ TF-IDF + Logistic Regression é€²è¡Œåˆ†é¡ã€‚")
                try:
                    vectorizer = model.named_steps["tfidf"]
                    vocab_size = len(vectorizer.vocabulary_)
                    st.write(f"ç›®å‰ TF-IDF è©å½™æ•¸é‡ï¼šç´„ **{vocab_size}** å€‹ç‰¹å¾µã€‚")
                except Exception:
                    st.write("ç„¡æ³•å–å¾— TF-IDF ç‰¹å¾µè³‡è¨Šã€‚")
